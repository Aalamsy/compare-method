{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba970312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alamsyah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alamsyah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing Quran texts...\n",
      "Loaded: en.ahmedali\n",
      "Loaded: en.ahmedraza\n",
      "Loaded: en.arberry\n",
      "Loaded: en.daryabadi\n",
      "Loaded: en.hilali\n",
      "Loaded: en.itani\n",
      "Loaded: en.maududi\n",
      "Loaded: en.mubarakpuri\n",
      "Loaded: en.pickthall\n",
      "Loaded: en.qarai\n",
      "Loaded: en.qaribullah\n",
      "Loaded: en.sahih\n",
      "Loaded: en.sarwar\n",
      "Loaded: en.shakir\n",
      "Loaded: en.transliteration\n",
      "Loaded: en.wahiduddin\n",
      "Loaded: en.yusufali\n",
      "Total unique words: 36254\n",
      "Top 20 most common words:\n",
      "allah: 32922\n",
      "god: 16415\n",
      "lord: 15867\n",
      "shall: 12084\n",
      "say: 11725\n",
      "said: 11444\n",
      "people: 9788\n",
      "day: 8585\n",
      "one: 7962\n",
      "indeed: 7399\n",
      "u: 7279\n",
      "know: 6291\n",
      "earth: 6199\n",
      "believe: 5882\n",
      "ye: 5727\n",
      "surely: 5602\n",
      "may: 5552\n",
      "come: 5467\n",
      "upon: 5430\n",
      "would: 5364\n",
      "\n",
      "Training word2vecLL model...\n",
      "\n",
      "Vocabulary size in trained Word2Vec model: 9998\n",
      "\n",
      "Saving word2vecLL model...\n",
      "word2vecLL model training and saving completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_quran_texts(base_path):\n",
    "    translations = [\n",
    "        'en.ahmedali', 'en.ahmedraza', 'en.arberry', 'en.daryabadi',\n",
    "        'en.hilali', 'en.itani', 'en.maududi', 'en.mubarakpuri',\n",
    "        'en.pickthall', 'en.qarai', 'en.qaribullah', 'en.sahih',\n",
    "        'en.sarwar', 'en.shakir', 'en.transliteration', 'en.wahiduddin',\n",
    "        'en.yusufali'\n",
    "    ]\n",
    "    all_texts = []\n",
    "    for translation in translations:\n",
    "        file_path = os.path.join(base_path, f\"{translation}.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                all_texts.append(text.lower())\n",
    "            print(f\"Loaded: {translation}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "    return all_texts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "def lemmatize_text(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def train_word2vec(texts, size=200, window=10, min_count=5, workers=4):\n",
    "    model = Word2Vec(sentences=texts, vector_size=size, window=window, min_count=min_count, workers=workers, sg=0)  # sg=0 for CBOW\n",
    "    return model\n",
    "def get_vocabulary(texts):\n",
    "    all_words = [word for text in texts for word in text]\n",
    "    vocab = Counter(all_words)\n",
    "    return vocab\n",
    "\n",
    "def print_vocabulary_stats(vocab):\n",
    "    print(f\"Total unique words: {len(vocab)}\")\n",
    "    print(f\"Top 20 most common words:\")\n",
    "    for word, count in vocab.most_common(20):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Loading and preprocessing Quran texts...\")\n",
    "    base_path = r'C:\\Users\\Alamsyah\\Skripsi 6\\Quran'  # Update this path\n",
    "    quran_texts = load_quran_texts(base_path)\n",
    "    \n",
    "    preprocessed_texts = [preprocess_text(text) for text in quran_texts]\n",
    "    lemmatized_texts = [lemmatize_text(text) for text in preprocessed_texts]\n",
    "    \n",
    "    # Get vocabulary after preprocessing\n",
    "    vocab = get_vocabulary(lemmatized_texts)\n",
    "    print_vocabulary_stats(vocab)\n",
    "    \n",
    "    print(\"\\nTraining word2vecLL model...\")\n",
    "    word2vecLL = train_word2vec(lemmatized_texts)\n",
    "    \n",
    "    # Print vocabulary size in the trained model\n",
    "    print(f\"\\nVocabulary size in trained Word2Vec model: {len(word2vecLL.wv.key_to_index)}\")\n",
    "    \n",
    "    print(\"\\nSaving word2vecLL model...\")\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    word2vecLL.save(\"models/word2vecLL_quran.model\")\n",
    "    \n",
    "    print(\"word2vecLL model training and saving completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea656a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vecLL model...\n",
      "Loading Golden Standard from 5w100c...\n",
      "Training word2vecLLS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alamsyah\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Mean Error: 0.012833860099951577\n",
      "Iteration 2, Mean Error: 0.012260762488352318\n",
      "Iteration 3, Mean Error: 0.012172185608659184\n",
      "Iteration 4, Mean Error: 0.012808869199219088\n",
      "Iteration 5, Mean Error: 0.012208642212833416\n",
      "Iteration 6, Mean Error: 0.01224649458319283\n",
      "Iteration 7, Mean Error: 0.012215524807038415\n",
      "Iteration 8, Mean Error: 0.012163781271080852\n",
      "Iteration 9, Mean Error: 0.012484550814821008\n",
      "Iteration 10, Mean Error: 0.012004132075545014\n",
      "Iteration 11, Mean Error: 0.011907030389243515\n",
      "Iteration 12, Mean Error: 0.01194123295653125\n",
      "Iteration 13, Mean Error: 0.011969077840810517\n",
      "Iteration 14, Mean Error: 0.011994461576880468\n",
      "Iteration 15, Mean Error: 0.012171330486600324\n",
      "Iteration 16, Mean Error: 0.011892232632189498\n",
      "Iteration 17, Mean Error: 0.012412023262820765\n",
      "Iteration 18, Mean Error: 0.012389229309364147\n",
      "Iteration 19, Mean Error: 0.011902741999840165\n",
      "Iteration 20, Mean Error: 0.012002346861300802\n",
      "Iteration 21, Mean Error: 0.012629826774130564\n",
      "Iteration 22, Mean Error: 0.011786552586762254\n",
      "Iteration 23, Mean Error: 0.011778002112463623\n",
      "Iteration 24, Mean Error: 0.011873296811203197\n",
      "Iteration 25, Mean Error: 0.011823262743065562\n",
      "Iteration 26, Mean Error: 0.012064846226525426\n",
      "Iteration 27, Mean Error: 0.011761410851688818\n",
      "Iteration 28, Mean Error: 0.012343676846371726\n",
      "Iteration 29, Mean Error: 0.01210768580095729\n",
      "Iteration 30, Mean Error: 0.011736142407902774\n",
      "Iteration 31, Mean Error: 0.011750747363254786\n",
      "Iteration 32, Mean Error: 0.012174353359256224\n",
      "Iteration 33, Mean Error: 0.012011964242738232\n",
      "Iteration 34, Mean Error: 0.011790108600298915\n",
      "Iteration 35, Mean Error: 0.011690493532347723\n",
      "Iteration 36, Mean Error: 0.011842055170305385\n",
      "Iteration 37, Mean Error: 0.012676948127905244\n",
      "Iteration 38, Mean Error: 0.011745697178869079\n",
      "Iteration 39, Mean Error: 0.011679506049420977\n",
      "Iteration 40, Mean Error: 0.011697942178065728\n",
      "Iteration 41, Mean Error: 0.01180748022140185\n",
      "Iteration 42, Mean Error: 0.01214991723841896\n",
      "Iteration 43, Mean Error: 0.011792125039332859\n",
      "Iteration 44, Mean Error: 0.01171927237509962\n",
      "Iteration 45, Mean Error: 0.011796501622671969\n",
      "Iteration 46, Mean Error: 0.011684533545664188\n",
      "Iteration 47, Mean Error: 0.011697612482930235\n",
      "Iteration 48, Mean Error: 0.011824358463421787\n",
      "Iteration 49, Mean Error: 0.012191071439474156\n",
      "Iteration 50, Mean Error: 0.011892135850237705\n",
      "Iteration 51, Mean Error: 0.01162843426748828\n",
      "Iteration 52, Mean Error: 0.011655726011393886\n",
      "Iteration 53, Mean Error: 0.011609162169156355\n",
      "Iteration 54, Mean Error: 0.011767589966265252\n",
      "Iteration 55, Mean Error: 0.011598204352941696\n",
      "Iteration 56, Mean Error: 0.011892616385350328\n",
      "Iteration 57, Mean Error: 0.011972685253401874\n",
      "Iteration 58, Mean Error: 0.011620458429903327\n",
      "Iteration 59, Mean Error: 0.011554676389189887\n",
      "Iteration 60, Mean Error: 0.01185521120487319\n",
      "Iteration 61, Mean Error: 0.011564896431793707\n",
      "Iteration 62, Mean Error: 0.011716430865609867\n",
      "Iteration 63, Mean Error: 0.01217169240699102\n",
      "Iteration 64, Mean Error: 0.011526398866823582\n",
      "Iteration 65, Mean Error: 0.012364667352666777\n",
      "Iteration 66, Mean Error: 0.01152942728128717\n",
      "Iteration 67, Mean Error: 0.011571633698980743\n",
      "Iteration 68, Mean Error: 0.011812908492347882\n",
      "Iteration 69, Mean Error: 0.011505376858793125\n",
      "Iteration 70, Mean Error: 0.011785196159795279\n",
      "Iteration 71, Mean Error: 0.011513709171877851\n",
      "Iteration 72, Mean Error: 0.011953070648113892\n",
      "Iteration 73, Mean Error: 0.011528191980563166\n",
      "Iteration 74, Mean Error: 0.01196604362318683\n",
      "Iteration 75, Mean Error: 0.013253592020222953\n",
      "Iteration 76, Mean Error: 0.011990645983148305\n",
      "Iteration 77, Mean Error: 0.01154528951992493\n",
      "Iteration 78, Mean Error: 0.011472602951346141\n",
      "Iteration 79, Mean Error: 0.011492325733404309\n",
      "Iteration 80, Mean Error: 0.011444400378143097\n",
      "Iteration 81, Mean Error: 0.011556497684845185\n",
      "Iteration 82, Mean Error: 0.0115414208805402\n",
      "Iteration 83, Mean Error: 0.011980709209241387\n",
      "Iteration 84, Mean Error: 0.011553865099356147\n",
      "Iteration 85, Mean Error: 0.011553008403634685\n",
      "Iteration 86, Mean Error: 0.011676927370432778\n",
      "Iteration 87, Mean Error: 0.011433181766767744\n",
      "Iteration 88, Mean Error: 0.011627724814088521\n",
      "Iteration 89, Mean Error: 0.01150191914554991\n",
      "Iteration 90, Mean Error: 0.011437644775110743\n",
      "Iteration 91, Mean Error: 0.011447128080411413\n",
      "Iteration 92, Mean Error: 0.01190413307050978\n",
      "Iteration 93, Mean Error: 0.0117369169286542\n",
      "Iteration 94, Mean Error: 0.011557863041816453\n",
      "Iteration 95, Mean Error: 0.011435030200392061\n",
      "Iteration 96, Mean Error: 0.011553264289594198\n",
      "Iteration 97, Mean Error: 0.011958275068458424\n",
      "Iteration 98, Mean Error: 0.011560380417124254\n",
      "Iteration 99, Mean Error: 0.011438288476640733\n",
      "Iteration 100, Mean Error: 0.011540459867366303\n",
      "Saving word2vecLLS model...\n",
      "word2vecLLS model training and saving completed.\n",
      "\n",
      "Vocabulary size in trained Word2Vec model: 9998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load the information content\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def safe_similarity(func):\n",
    "    def wrapper(word1, word2):\n",
    "        try:\n",
    "            return func(word1, word2)\n",
    "        except:\n",
    "            return 0\n",
    "    return wrapper\n",
    "\n",
    "@safe_similarity\n",
    "def wu_palmer_similarity(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if synsets1 and synsets2:\n",
    "        max_sim = max((s1.wup_similarity(s2) or 0) for s1 in synsets1 for s2 in synsets2)\n",
    "        return max_sim if max_sim > 0 else 0\n",
    "    return 0\n",
    "\n",
    "@safe_similarity\n",
    "def jiang_conrath_similarity(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if synsets1 and synsets2:\n",
    "        max_sim = max((s1.jcn_similarity(s2, brown_ic) or 0) for s1 in synsets1 for s2 in synsets2)\n",
    "        return max_sim if max_sim > 0 else 0\n",
    "    return 0\n",
    "\n",
    "@safe_similarity\n",
    "def hirst_st_onge_similarity(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if synsets1 and synsets2:\n",
    "        max_sim = max((s1.hso_similarity(s2) or 0) for s1 in synsets1 for s2 in synsets2)\n",
    "        return max_sim if max_sim > 0 else 0\n",
    "    return 0\n",
    "\n",
    "def create_similarity_matrix(word, similar_words):\n",
    "    M = np.zeros((4, len(similar_words)))\n",
    "    for i, similar_word in enumerate(similar_words):\n",
    "        M[0, i] = wu_palmer_similarity(word, similar_word)\n",
    "        M[1, i] = jiang_conrath_similarity(word, similar_word)\n",
    "        M[2, i] = hirst_st_onge_similarity(word, similar_word)\n",
    "        M[3, i] = 1  # bias\n",
    "    return M\n",
    "    print(f\"Similarity matrix for '{word}':\")\n",
    "    print(M)\n",
    "    return M\n",
    "\n",
    "def normalize_matrix(M):\n",
    "    min_vals = M.min(axis=1, keepdims=True)\n",
    "    max_vals = M.max(axis=1, keepdims=True)\n",
    "    return (M - min_vals) / (max_vals - min_vals + 1e-10)\n",
    "    print(\"Normalized similarity matrix:\")\n",
    "    print(normalized_M)\n",
    "    return normalized_M\n",
    "\n",
    "def create_value_matrix(D, SM):\n",
    "    return np.vstack((D, SM)).T\n",
    "    print(\"Value matrix:\")\n",
    "    print(V)\n",
    "    return V\n",
    "\n",
    "def seek(w, P):\n",
    "    try:\n",
    "        return np.where(P[0] == w)[0][0]\n",
    "    except IndexError:\n",
    "        return P.shape[1]\n",
    "\n",
    "def calculate_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def load_golden_standard(file_path):\n",
    "    golden_standard = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) == 2:\n",
    "                concept = parts[0].strip()\n",
    "                words = [w.strip() for w in parts[1].split(',')][:5]\n",
    "                golden_standard[concept] = words\n",
    "    return golden_standard\n",
    "\n",
    "def train_word2vecLLS(word2vecLL, golden_standard, max_iter=100):\n",
    "    nn_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1, warm_start=True)\n",
    "    all_words = list(word2vecLL.wv.key_to_index.keys())\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for concept, related_words in golden_standard.items():\n",
    "        if concept not in word2vecLL.wv:\n",
    "            continue\n",
    "        similar_words = [w for w, _ in word2vecLL.wv.most_similar(concept, topn=20)]\n",
    "        M = create_similarity_matrix(concept, similar_words)\n",
    "        SM = normalize_matrix(M)\n",
    "        D = word2vecLL.wv.distances(concept, similar_words)\n",
    "        V = create_value_matrix(D, SM)\n",
    "        X_train.extend(V)\n",
    "        y_train.extend([1 if word in related_words else 0 for word in similar_words])\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        nn_model.fit(X_train, y_train)\n",
    "        y_pred = nn_model.predict(X_train)\n",
    "        error = mean_squared_error(y_train, y_pred)\n",
    "        print(f\"Iteration {i+1}, Mean Error: {error}\")\n",
    "        \n",
    "        if error < 0.01:\n",
    "            break\n",
    "    \n",
    "    return nn_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading word2vecLL model...\")\n",
    "    word2vecLL = Word2Vec.load(\"models/word2vecLL_quran.model\")\n",
    "    \n",
    "    print(\"Loading Golden Standard from 5w100c...\")\n",
    "    golden_standard = load_golden_standard(\"5w100c.txt\")\n",
    "    \n",
    "    print(\"Training word2vecLLS model...\")\n",
    "    word2vecLLS = train_word2vecLLS(word2vecLL, golden_standard, max_iter=100)\n",
    "    \n",
    "    print(\"Saving word2vecLLS model...\")\n",
    "    joblib.dump(word2vecLLS, \"models/word2vecLLS_quran.model\")\n",
    "    print(\"word2vecLLS model training and saving completed.\")\n",
    "    print(f\"\\nVocabulary size in trained Word2Vec model: {len(word2vecLL.wv.key_to_index)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485d5d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loading 5w100c dataset...\n",
      "Evaluating word2vecLL model...\n",
      "Skipped concepts: ['children', 'orphans', 'neighbors', 'travelers', 'strangers', 'oppressors', 'unity', 'disunity']\n",
      "Number of skipped concepts: 8\n",
      "Evaluating word2vecLLS model...\n",
      "Skipped concepts: ['children', 'orphans', 'neighbors', 'travelers', 'strangers', 'oppressors', 'unity', 'disunity']\n",
      "Number of skipped concepts: 8\n",
      "\n",
      "Results for k=20:\n",
      "word2vecLL - Precision: 0.0118, Recall: 0.0471, F1: 0.0188\n",
      "word2vecLLS - Precision: 0.0118, Recall: 0.0471, F1: 0.0188\n",
      "\n",
      "Results for k=50:\n",
      "word2vecLL - Precision: 0.0054, Recall: 0.0541, F1: 0.0098\n",
      "word2vecLLS - Precision: 0.0054, Recall: 0.0541, F1: 0.0098\n",
      "\n",
      "Results for k=100:\n",
      "word2vecLL - Precision: 0.0047, Recall: 0.0941, F1: 0.0090\n",
      "word2vecLLS - Precision: 0.0047, Recall: 0.0941, F1: 0.0090\n",
      "\n",
      "Results for k=200:\n",
      "word2vecLL - Precision: 0.0031, Recall: 0.1247, F1: 0.0061\n",
      "word2vecLLS - Precision: 0.0031, Recall: 0.1247, F1: 0.0061\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "def load_5w100c(file_path):\n",
    "    word_groups = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) == 2:\n",
    "                concept = parts[0].strip()\n",
    "                words = [w.strip() for w in parts[1].split(',')][:5]  # Take only the first 5 words\n",
    "                word_groups[concept] = words\n",
    "    return word_groups\n",
    "\n",
    "def evaluate_model(model_func, golden_standard, k_values=[20, 50, 100, 200]):\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        skipped_concepts = []\n",
    "        \n",
    "        for concept, related_words in golden_standard.items():\n",
    "            try:\n",
    "                predicted_similar = [w for w, _ in model_func(concept, topn=k)]\n",
    "            except KeyError:\n",
    "                skipped_concepts.append(concept)\n",
    "                continue\n",
    "            \n",
    "            true_positives += len(set(related_words) & set(predicted_similar))\n",
    "            false_positives += len(predicted_similar) - len(set(related_words) & set(predicted_similar))\n",
    "            false_negatives += len(set(related_words) - set(predicted_similar))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results[k] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    print(f\"Skipped concepts: {skipped_concepts}\")\n",
    "    print(f\"Number of skipped concepts: {len(skipped_concepts)}\")\n",
    "    return results\n",
    "\n",
    "# In your main function, update the evaluation part:\n",
    "def main():\n",
    "    print(\"Loading models...\")\n",
    "    word2vecLL = Word2Vec.load(\"models/word2vecLL_quran.model\")\n",
    "    word2vecLLS = joblib.load(\"models/word2vecLLS_quran.model\")\n",
    "    \n",
    "    print(\"Loading 5w100c dataset...\")\n",
    "    golden_standard = load_golden_standard('5w100c.txt')\n",
    "    \n",
    "    print(\"Evaluating word2vecLL model...\")\n",
    "    results_LL = evaluate_model(word2vecLL.wv.most_similar, golden_standard)\n",
    "    \n",
    "    print(\"Evaluating word2vecLLS model...\")\n",
    "    def word2vecLLS_most_similar(word, topn):\n",
    "        similar_words = [w for w, _ in word2vecLL.wv.most_similar(word, topn=topn)]\n",
    "        M = create_similarity_matrix(word, similar_words)\n",
    "        SM = normalize_matrix(M)\n",
    "        D = word2vecLL.wv.distances(word, similar_words)\n",
    "        V = create_value_matrix(D, SM)\n",
    "        similarities = word2vecLLS.predict(V)\n",
    "        return sorted(zip(similar_words, similarities), key=lambda x: x[1], reverse=True)[:topn]\n",
    "    \n",
    "    results_LLS = evaluate_model(word2vecLLS_most_similar, golden_standard)\n",
    "    \n",
    "    # Print results\n",
    "    for k in [20, 50, 100, 200]:\n",
    "        print(f\"\\nResults for k={k}:\")\n",
    "        print(f\"word2vecLL - Precision: {results_LL[k]['precision']:.4f}, Recall: {results_LL[k]['recall']:.4f}, F1: {results_LL[k]['f1']:.4f}\")\n",
    "        print(f\"word2vecLLS - Precision: {results_LLS[k]['precision']:.4f}, Recall: {results_LLS[k]['recall']:.4f}, F1: {results_LLS[k]['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100736eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433e1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
